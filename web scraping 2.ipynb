{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1396d3-f687-4951-bd96-1582bab41ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'clear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Enter \"Data Scientist\" in the search field and click search button\u001b[39;00m\n\u001b[0;32m     11\u001b[0m search_box \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqsb-keyword-sugg\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 12\u001b[0m search_box\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     13\u001b[0m search_box\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Scientist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m search_button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtn-primary\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clear'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.naukri.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field and click search button\n",
    "search_box = soup.find('input', {'id': 'qsb-keyword-sugg'})\n",
    "search_box.clear()\n",
    "search_box.send_keys('Data Scientist')\n",
    "\n",
    "search_button = soup.find('button', {'class': 'btn-primary'})\n",
    "search_button.click()\n",
    "\n",
    "# Step 3: Apply location and salary filters\n",
    "location_filter = soup.find('label', text='Delhi / NCR')\n",
    "location_filter_checkbox = location_filter.find('i', {'class': 'fleft check'})\n",
    "location_filter_checkbox.click()\n",
    "\n",
    "salary_filter = soup.find('label', text='3-6 Lakhs')\n",
    "salary_filter_checkbox = salary_filter.find('i', {'class': 'fleft check'})\n",
    "salary_filter_checkbox.click()\n",
    "\n",
    "# Step 4: Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "job_results = soup.find_all('article', {'class': 'jobTuple'})\n",
    "\n",
    "for job in job_results[:10]:\n",
    "    job_title = job.find('a', {'class': 'title'}).text.strip()\n",
    "    job_location = job.find('li', {'class': 'location'}).text.strip()\n",
    "    company_name = job.find('a', {'class': 'subTitle'}).text.strip()\n",
    "    experience_required = job.find('li', {'class': 'experience'}).text.strip()\n",
    "    \n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3a7d1c-e355-4c26-ad34-5b9ad1aa7f18",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'clear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Enter job title and location, then click search button\u001b[39;00m\n\u001b[0;32m     11\u001b[0m search_job_input \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt_search\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 12\u001b[0m search_job_input\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     13\u001b[0m search_job_input\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Analyst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m search_location_input \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clear'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Enter job title and location, then click search button\n",
    "search_job_input = soup.find('input', {'id': 'txt_search'})\n",
    "search_job_input.clear()\n",
    "search_job_input.send_keys('Data Analyst')\n",
    "\n",
    "search_location_input = soup.find('input', {'id': 'city'})\n",
    "search_location_input.clear()\n",
    "search_location_input.send_keys('Bangalore')\n",
    "\n",
    "search_button = soup.find('button', {'class': 'btn btn-success'})\n",
    "search_button.click()\n",
    "\n",
    "# Step 3: Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "job_results = soup.find_all('li', {'class': 'search_listing'})\n",
    "\n",
    "for job in job_results[:10]:\n",
    "    job_title = job.find('a', {'class': 'cls_searchresult_a'}).text.strip()\n",
    "    job_location = job.find('span', {'class': 'txt_lgt_gray'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'comp_name'}).text.strip()\n",
    "    experience_required = job.find('li', {'class': 'exp'}).text.strip()\n",
    "    \n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "# Step 4: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Step 5: Display DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3229b9-85cc-4bc2-8fdf-5f4562169e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Saved to iphone11_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews from a given URL\n",
    "def scrape_reviews(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    reviews = []\n",
    "\n",
    "    # Find all review blocks\n",
    "    review_blocks = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "    for review_block in review_blocks:\n",
    "        # Find rating\n",
    "        rating = review_block.find('div', {'class': '_3LWZlK'}).text.strip()\n",
    "\n",
    "        # Find review summary\n",
    "        review_summary = review_block.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "\n",
    "        # Find full review\n",
    "        full_review = review_block.find('div', {'class': 't-ZTKy'}).text.strip()\n",
    "\n",
    "        reviews.append({\n",
    "            'Rating': rating,\n",
    "            'Review Summary': review_summary,\n",
    "            'Full Review': full_review\n",
    "        })\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Main function to scrape 100 reviews\n",
    "def main():\n",
    "    base_url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&page='\n",
    "    reviews = []\n",
    "\n",
    "    for page in range(1, 11):  # There are 10 pages with 10 reviews each\n",
    "        url = base_url + str(page)\n",
    "        reviews += scrape_reviews(url)\n",
    "        if len(reviews) >= 100:\n",
    "            break\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(reviews)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('iphone11_reviews.csv', index=False)\n",
    "\n",
    "    print('Scraping complete. Saved to iphone11_reviews.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e308a579-e0a7-4af1-bec8-a5377bcd3e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Saved to sneakers_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for sneakers\n",
    "def scrape_sneakers():\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "    search_url = base_url + '/search?q=sneakers'\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    sneakers_data = []\n",
    "\n",
    "    # Find all sneaker product elements\n",
    "    sneaker_elements = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    for sneaker in sneaker_elements[:100]:\n",
    "        # Find brand\n",
    "        brand = sneaker.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "\n",
    "        # Find product description\n",
    "        description = sneaker.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "\n",
    "        # Find price\n",
    "        price = sneaker.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "        sneakers_data.append({\n",
    "            'Brand': brand,\n",
    "            'Product Description': description,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return sneakers_data\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    sneakers_data = scrape_sneakers()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(sneakers_data)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('sneakers_data.csv', index=False)\n",
    "\n",
    "    print('Scraping complete. Saved to sneakers_data.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b06c4f0-e156-44cd-8699-b685fb1aee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Saved to laptops_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for laptops\n",
    "def scrape_laptops():\n",
    "    base_url = 'https://www.amazon.in'\n",
    "    search_url = base_url + '/s?k=Laptop'\n",
    "\n",
    "    # Add search parameters for CPU type filter\n",
    "    params = {'rh': 'n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031'}\n",
    "    response = requests.get(search_url, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    laptops_data = []\n",
    "\n",
    "    # Find all laptop product elements\n",
    "    laptop_elements = soup.find_all('div', {'class': 's-result-item'})\n",
    "\n",
    "    for laptop in laptop_elements[:10]:\n",
    "        # Find title\n",
    "        title = laptop.find('span', {'class': 'a-text-normal'}).text.strip()\n",
    "\n",
    "        # Find ratings\n",
    "        ratings_tag = laptop.find('span', {'class': 'a-icon-alt'})\n",
    "        ratings = ratings_tag.text.strip() if ratings_tag else 'Not available'\n",
    "\n",
    "        # Find price\n",
    "        price_tag = laptop.find('span', {'class': 'a-price-whole'})\n",
    "        price = price_tag.text.strip() if price_tag else 'Not available'\n",
    "\n",
    "        laptops_data.append({\n",
    "            'Title': title,\n",
    "            'Ratings': ratings,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return laptops_data\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    laptops_data = scrape_laptops()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(laptops_data)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('laptops_data.csv', index=False)\n",
    "\n",
    "    print('Scraping complete. Saved to laptops_data.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81cf0c9-3f23-4400-89e1-0be59e9e9902",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to scrape data for top 1000 quotes\n",
    "def scrape_quotes():\n",
    "    # Start a Selenium WebDriver session\n",
    "    driver = webdriver.Chrome()  # You need to have chromedriver installed and its path added to PATH environment variable\n",
    "    driver.get('https://www.azquotes.com/')\n",
    "\n",
    "    # Click on Top Quotes\n",
    "    top_quotes_link = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, 'Top Quotes')))\n",
    "    top_quotes_link.click()\n",
    "\n",
    "    # Scrape quotes data\n",
    "    quotes_data = []\n",
    "    while len(quotes_data) < 1000:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        quote_elements = soup.find_all('div', {'class': 'title'})\n",
    "\n",
    "        for quote_element in quote_elements:\n",
    "            quote = quote_element.text.strip()\n",
    "            author = quote_element.find_next_sibling('div', {'class': 'author'}).text.strip()\n",
    "            quote_type = quote_element.find_next_sibling('div', {'class': 'auteur'}).text.strip()\n",
    "\n",
    "            quotes_data.append({\n",
    "                'Quote': quote,\n",
    "                'Author': author,\n",
    "                'Type Of Quotes': quote_type\n",
    "            })\n",
    "\n",
    "        # Scroll down to load more quotes\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.CLASS_NAME, 'inf-loading')))\n",
    "\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n",
    "\n",
    "    return quotes_data[:1000]  # Return only the first 1000 quotes\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    quotes_data = scrape_quotes()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(quotes_data)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('top_1000_quotes.csv', index=False)\n",
    "\n",
    "    print('Scraping complete. Saved to top_1000_quotes.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2adadb1-eeb6-4a1f-9ff2-440efc6a5146",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 36\u001b[0m     prime_ministers_data \u001b[38;5;241m=\u001b[39m scrape_prime_ministers()\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(prime_ministers_data)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mscrape_prime_ministers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable table-striped\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Extract data from the table rows\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[0;32m     19\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for former Prime Ministers of India\n",
    "def scrape_prime_ministers():\n",
    "    url = 'https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    prime_ministers_data = []\n",
    "\n",
    "    # Find the table containing the information\n",
    "    table = soup.find('table', {'class': 'table table-striped'})\n",
    "\n",
    "    # Extract data from the table rows\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        born_dead = columns[1].text.strip()\n",
    "        term_of_office = columns[2].text.strip()\n",
    "        remarks = columns[3].text.strip()\n",
    "\n",
    "        prime_ministers_data.append({\n",
    "            'Name': name,\n",
    "            'Born-Dead': born_dead,\n",
    "            'Term of Office': term_of_office,\n",
    "            'Remarks': remarks\n",
    "        })\n",
    "\n",
    "    return prime_ministers_data\n",
    "\n",
    "# Main function to scrape and display data as DataFrame\n",
    "def main():\n",
    "    prime_ministers_data = scrape_prime_ministers()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(prime_ministers_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65337ba7-0485-4c75-a44e-d6f823bc53e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article link not found.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_4192\\3835102624.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  article_link = soup.find('a', text='50 Most Expensive Cars in the World')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for the 50 most expensive cars\n",
    "def scrape_expensive_cars():\n",
    "    url = 'https://www.motor1.com/'\n",
    "    search_query = '50 most expensive cars'\n",
    "    \n",
    "    # Perform search\n",
    "    response = requests.get(url, params={'q': search_query})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the link to the article\n",
    "    article_link = soup.find('a', text='50 Most Expensive Cars in the World')\n",
    "    if article_link:\n",
    "        article_url = article_link['href']\n",
    "    else:\n",
    "        print(\"Article link not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Access the article page\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the list of cars\n",
    "    cars_data = []\n",
    "    cars_list = soup.find('ol', {'class': 'gallery-list'})\n",
    "    if cars_list:\n",
    "        cars = cars_list.find_all('li')\n",
    "        for car in cars:\n",
    "            car_name = car.find('h3').text.strip()\n",
    "            car_price = car.find('p').text.strip()\n",
    "            cars_data.append({\n",
    "                'Car Name': car_name,\n",
    "                'Price': car_price\n",
    "            })\n",
    "    else:\n",
    "        print(\"List of cars not found.\")\n",
    "    \n",
    "    return cars_data\n",
    "\n",
    "# Main function to scrape and display data as DataFrame\n",
    "def main():\n",
    "    expensive_cars_data = scrape_expensive_cars()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(expensive_cars_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba17647-e8df-47b2-9e4c-15ee3ac25c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
