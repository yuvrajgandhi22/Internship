{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fcacaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name  Rating  Year\n",
      "0                     Ship of Theseus     8.0  2012\n",
      "1                              Iruvar     8.4  1997\n",
      "2                     Kaagaz Ke Phool     7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India     8.1  2001\n",
      "4                     Pather Panchali     8.2  1955\n",
      "..                                ...     ...   ...\n",
      "95                        Apur Sansar     8.4  1959\n",
      "96                        Kanchivaram     8.2  2008\n",
      "97                    Monsoon Wedding     7.3  2001\n",
      "98                              Black     8.1  2005\n",
      "99                            Deewaar     8.0  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        movie_container = soup.find('div', class_='lister-list')\n",
    "        \n",
    "        names = []\n",
    "        ratings = []\n",
    "        years = []\n",
    "        \n",
    "        for movie in movie_container.find_all('div', class_='lister-item-content'):\n",
    "            name = movie.h3.a.text.strip()\n",
    "            names.append(name)\n",
    "            \n",
    "            rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "            ratings.append(float(rating))\n",
    "            \n",
    "            year = movie.h3.find('span', class_='lister-item-year').text.strip('()')\n",
    "            years.append(year)\n",
    "        \n",
    "        df = pd.DataFrame({'Name': names, 'Rating': ratings, 'Year': years})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "imdb_top_100_df = scrape_imdb_top_100_indian_movies(url)\n",
    "print(imdb_top_100_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a61569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Product Name, Price, Discount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_peachmode_products(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        product_container = soup.find_all('div', class_='product-item')\n",
    "        \n",
    "        names = []\n",
    "        prices = []\n",
    "        discounts = []\n",
    "        \n",
    "        for product in product_container:\n",
    "            name = product.find('a', class_='product-title').text.strip()\n",
    "            names.append(name)\n",
    "            \n",
    "            price = product.find('span', class_='money').text.strip()\n",
    "            prices.append(price)\n",
    "            \n",
    "            discount_tag = product.find('span', class_='discount')\n",
    "            discount = discount_tag.text.strip() if discount_tag else 'No Discount'\n",
    "            discounts.append(discount)\n",
    "        \n",
    "        df = pd.DataFrame({'Product Name': names, 'Price': prices, 'Discount': discounts})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://peachmode.com/search?q=bags\"\n",
    "peachmode_products_df = scrape_peachmode_products(url)\n",
    "print(peachmode_products_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878a7faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODI Teams Rankings:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "ODI Batsmen Rankings:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "ODI Bowlers Rankings:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_teams_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        teams_data = []\n",
    "        for team in soup.find_all('tr', class_='rankings-block__banner')[:1] + soup.find_all('tr', class_='table-body')[:9]:\n",
    "            team_name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "            matches = team.find_all('td', class_='table-body__cell')[0].text.strip()\n",
    "            points = team.find_all('td', class_='table-body__cell')[1].text.strip()\n",
    "            rating = team.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "            \n",
    "            teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "        \n",
    "        df = pd.DataFrame(teams_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def scrape_odi_batsmen_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        batsmen_data = []\n",
    "        for batsman in soup.find_all('tr', class_='rankings-block__banner')[:1] + soup.find_all('tr', class_='table-body')[:9]:\n",
    "            player_name = batsman.find('div', class_='rankings-block__banner--name').text.strip()\n",
    "            team = batsman.find('div', class_='rankings-block__banner--nationality').text.strip()\n",
    "            rating = batsman.find('div', class_='rankings-block__banner--rating').text.strip()\n",
    "            \n",
    "            batsmen_data.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "        \n",
    "        df = pd.DataFrame(batsmen_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def scrape_odi_bowlers_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        bowlers_data = []\n",
    "        for bowler in soup.find_all('tr', class_='rankings-block__banner')[:1] + soup.find_all('tr', class_='table-body')[:9]:\n",
    "            player_name = bowler.find('div', class_='rankings-block__banner--name').text.strip()\n",
    "            team = bowler.find('div', class_='rankings-block__banner--nationality').text.strip()\n",
    "            rating = bowler.find('div', class_='rankings-block__banner--rating').text.strip()\n",
    "            \n",
    "            bowlers_data.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "        \n",
    "        df = pd.DataFrame(bowlers_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "odi_teams_df = scrape_odi_teams_rankings(odi_teams_url)\n",
    "odi_batsmen_df = scrape_odi_batsmen_rankings(odi_batsmen_url)\n",
    "odi_bowlers_df = scrape_odi_bowlers_rankings(odi_bowlers_url)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"ODI Teams Rankings:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "print(\"\\nODI Batsmen Rankings:\")\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "print(\"\\nODI Bowlers Rankings:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ac8575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patreon Posts:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_patreon_posts(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        posts_data = []\n",
    "        \n",
    "        # Iterate through each post\n",
    "        for post in soup.find_all('div', class_='post-container'):\n",
    "            # Get post heading\n",
    "            heading = post.find('h2', class_='post-title').text.strip()\n",
    "            \n",
    "            # Get post date\n",
    "            date = post.find('time', class_='post-date')['datetime'].strip()\n",
    "            \n",
    "            # Get post content\n",
    "            content = post.find('div', class_='post-description').text.strip()\n",
    "            \n",
    "            # Get the link for the YouTube video\n",
    "            youtube_link = post.find('a', class_='post-attachment')['href']\n",
    "            \n",
    "            # Extract likes for the associated YouTube video\n",
    "            youtube_likes = scrape_youtube_likes(youtube_link)\n",
    "            \n",
    "            posts_data.append({'Heading': heading, 'Date': date, 'Content': content, 'YouTube Likes': youtube_likes})\n",
    "        \n",
    "        df = pd.DataFrame(posts_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def scrape_youtube_likes(youtube_link):\n",
    "    # Send a GET request to the YouTube link\n",
    "    response = requests.get(youtube_link)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract likes from the YouTube page\n",
    "        likes = soup.find('button', class_='yt-uix-button-icon yt-uix-button-icon-subscribe yt-uix-button-icon-large').text.strip()\n",
    "        \n",
    "        return likes\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve YouTube likes. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "patreon_url = \"https://www.patreon.com/coreyms\"\n",
    "patreon_posts_df = scrape_patreon_posts(patreon_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Patreon Posts:\")\n",
    "print(patreon_posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0d9b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nobroker Houses Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_nobroker_houses(url, localities):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        houses_data = []\n",
    "        \n",
    "        for locality in localities:\n",
    "            locality_url = f\"{url}property/sale/{locality}\"\n",
    "            locality_response = requests.get(locality_url)\n",
    "            \n",
    "            if locality_response.status_code == 200:\n",
    "                locality_soup = BeautifulSoup(locality_response.text, 'html.parser')\n",
    "                \n",
    "                for house in locality_soup.find_all('div', class_='card'):\n",
    "                    title = house.find('h2', class_='heading-6').text.strip()\n",
    "                    \n",
    "                    location = house.find('div', class_='nb__2NPHR').text.strip()\n",
    "                    \n",
    "                    area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "                    \n",
    "                    emi_tag = house.find('div', class_='font-semi-bold heading-6', text='₹')\n",
    "                    emi = emi_tag.text.strip() if emi_tag else 'N/A'\n",
    "                    \n",
    "                    price = house.find('div', class_='nb__2NPHR').find_next_sibling('div', class_='font-semi-bold heading-6').text.strip()\n",
    "                    \n",
    "                    houses_data.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n",
    "            \n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for {locality}. Status code: {locality_response.status_code}\")\n",
    "        \n",
    "        df = pd.DataFrame(houses_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "nobroker_url = \"https://www.nobroker.in/\"\n",
    "localities_list = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "\n",
    "nobroker_houses_df = scrape_nobroker_houses(nobroker_url, localities_list)\n",
    "\n",
    "print(\"Nobroker Houses Details:\")\n",
    "print(nobroker_houses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44dc5047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bewakoof Bestsellers:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_bewakoof_bestsellers(url, num_products=10):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        # Iterate through the first 10 product containers\n",
    "        for product in soup.find_all('div', class_='productCardWrapper')[:num_products]:\n",
    "            # Get product name\n",
    "            name = product.find('div', class_='productCardDetail').h3.text.strip()\n",
    "            \n",
    "            # Get product price\n",
    "            price = product.find('div', class_='productCardDetail').find('span', class_='originalPrice').text.strip()\n",
    "            \n",
    "            # Get product image URL\n",
    "            image_url = product.find('div', class_='productCardImgWrapper').img['src']\n",
    "            \n",
    "            products_data.append({'Name': name, 'Price': price, 'Image URL': image_url})\n",
    "        \n",
    "        df = pd.DataFrame(products_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "bewakoof_url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "bewakoof_bestsellers_df = scrape_bewakoof_bestsellers(bewakoof_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Bewakoof Bestsellers:\")\n",
    "print(bewakoof_bestsellers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08332ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNBC World Headlines:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_headlines(url, num_headlines=5):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        headlines_data = []\n",
    "        \n",
    "        for headline in soup.find_all('li', class_='CardList-story'):\n",
    "            heading = headline.find('div', class_='Card-headline').text.strip()\n",
    "            \n",
    "            date = headline.find('time', class_='Card-timestamp').text.strip()\n",
    "            \n",
    "            news_link = headline.find('a')['href']\n",
    "            \n",
    "            headlines_data.append({'Heading': heading, 'Date': date, 'News Link': news_link})\n",
    "            \n",
    "            if len(headlines_data) == num_headlines:\n",
    "                break\n",
    "        \n",
    "        df = pd.DataFrame(headlines_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "cnbc_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "cnbc_headlines_df = scrape_cnbc_headlines(cnbc_url, num_headlines=5)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"CNBC World Headlines:\")\n",
    "print(cnbc_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334583fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles in Artificial Intelligence in Agriculture:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url, num_articles=5):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        articles_data = []\n",
    "        \n",
    "        # Iterate through the first 5 articles\n",
    "        for article in soup.find_all('div', class_='article-info')[:num_articles]:\n",
    "            # Get paper title\n",
    "            title = article.find('h5', class_='card-title').text.strip()\n",
    "            \n",
    "            # Get date\n",
    "            date = article.find('span', class_='meta-date').text.strip()\n",
    "            \n",
    "            # Get author\n",
    "            author = article.find('span', class_='meta-author').text.strip()\n",
    "            \n",
    "            articles_data.append({'Paper Title': title, 'Date': date, 'Author': author})\n",
    "        \n",
    "        df = pd.DataFrame(articles_data)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "keaipublishing_url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "most_downloaded_articles_df = scrape_most_downloaded_articles(keaipublishing_url, num_articles=5)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Most Downloaded Articles in Artificial Intelligence in Agriculture:\")\n",
    "print(most_downloaded_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb89ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
