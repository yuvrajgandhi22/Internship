{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8193e82-1b06-4c95-8aaf-de0dd28a12db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "\n",
    "    if table:\n",
    "        rank_list = []\n",
    "        name_list = []\n",
    "        artist_list = []\n",
    "        upload_date_list = []\n",
    "        views_list = []\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            artist = columns[2].text.strip()\n",
    "            upload_date = columns[3].text.strip()\n",
    "            views = columns[4].text.strip()\n",
    "\n",
    "            rank_list.append(rank)\n",
    "            name_list.append(name)\n",
    "            artist_list.append(artist)\n",
    "            upload_date_list.append(upload_date)\n",
    "            views_list.append(views)\n",
    "\n",
    "        for i in range(len(rank_list)):\n",
    "            print(f\"Rank: {rank_list[i]}\")\n",
    "            print(f\"Name: {name_list[i]}\")\n",
    "            print(f\"Artist: {artist_list[i]}\")\n",
    "            print(f\"Upload Date: {upload_date_list[i]}\")\n",
    "            print(f\"Views: {views_list[i]}\")\n",
    "            print(\"---------------------------\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error occurred during HTTP request:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5240211f-145a-4e16-a2ab-b01e2d63dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to international fixtures page not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI website\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "try:\n",
    "    # Sending a GET request to the BCCI website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Checking if request was successful\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Finding the link to the international fixtures page\n",
    "    international_fixtures_link = soup.find(\"a\", {\"title\": \"International Fixtures\"})\n",
    "\n",
    "    if international_fixtures_link:\n",
    "        # Constructing the URL for the international fixtures page\n",
    "        international_fixtures_url = \"https://www.bcci.tv\" + international_fixtures_link[\"href\"]\n",
    "\n",
    "        # Sending a GET request to the international fixtures page\n",
    "        response_intl_fixtures = requests.get(international_fixtures_url)\n",
    "\n",
    "        # Checking if request was successful\n",
    "        response_intl_fixtures.raise_for_status()\n",
    "\n",
    "        # Parsing the HTML content of the international fixtures page\n",
    "        soup_intl_fixtures = BeautifulSoup(response_intl_fixtures.text, 'html.parser')\n",
    "\n",
    "        # Finding the elements containing fixture details\n",
    "        fixtures = soup_intl_fixtures.find_all(\"div\", class_=\"fixture__info\")\n",
    "\n",
    "        if fixtures:\n",
    "            print(\"Team India's International Fixtures:\")\n",
    "            print(\"------------------------------------\")\n",
    "            for fixture in fixtures:\n",
    "                series = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "                place = fixture.find(\"span\", class_=\"u-unskewed-text\").text.strip()\n",
    "                date = fixture.find(\"div\", class_=\"fixture__full-date\").text.strip()\n",
    "                time = fixture.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "\n",
    "                print(\"Series:\", series)\n",
    "                print(\"Place:\", place)\n",
    "                print(\"Date:\", date)\n",
    "                print(\"Time:\", time)\n",
    "                print(\"---------------------------\")\n",
    "\n",
    "        else:\n",
    "            print(\"No international fixtures found on the page.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Link to international fixtures page not found.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error occurred during HTTP request:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2b5894-3e67-468f-8b92-863aa4ff49a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to handle commonly occurring exceptions\n",
    "def handle_exceptions(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to scrape GDP data from statisticstimes.com\n",
    "@handle_exceptions\n",
    "def scrape_gdp_data(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Navigate to the economy page\n",
    "    driver.find_element(By.LINK_TEXT, \"Economy\").click()\n",
    "\n",
    "    # Navigate to the GDP of Indian states page\n",
    "    driver.find_element(By.LINK_TEXT, \"GDP of Indian states\").click()\n",
    "\n",
    "    # Extract HTML content\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find the table containing the GDP data\n",
    "    table = soup.find('table', {'id': 'table_id'})\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    rank_list = []\n",
    "    state_list = []\n",
    "    gsdp_1819_list = []\n",
    "    gsdp_1920_list = []\n",
    "    share_1819_list = []\n",
    "    gdp_billion_list = []\n",
    "\n",
    "    # Extract data from the table\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        rank_list.append(columns[0].text.strip())\n",
    "        state_list.append(columns[1].text.strip())\n",
    "        gsdp_1819_list.append(columns[2].text.strip())\n",
    "        gsdp_1920_list.append(columns[3].text.strip())\n",
    "        share_1819_list.append(columns[4].text.strip())\n",
    "        gdp_billion_list.append(columns[5].text.strip())\n",
    "\n",
    "    # Print the scraped data\n",
    "    print(\"Rank\\tState\\tGSDP(18-19)\\tGSDP(19-20)\\tShare(18-19)\\tGDP($ billion)\")\n",
    "    for i in range(len(rank_list)):\n",
    "        print(f\"{rank_list[i]}\\t{state_list[i]}\\t{gsdp_1819_list[i]}\\t{gsdp_1920_list[i]}\\t{share_1819_list[i]}\\t{gdp_billion_list[i]}\")\n",
    "\n",
    "# URL of the website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Call the function to scrape GDP data\n",
    "scrape_gdp_data(url)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a91f593-7e00-47ac-a19a-4f15c6df1f15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to handle commonly occurring exceptions\n",
    "def handle_exceptions(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to scrape trending repositories from GitHub\n",
    "@handle_exceptions\n",
    "def scrape_trending_repositories(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Click on the Explore menu and then on Trending\n",
    "    explore_menu = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//summary[contains(text(), 'Explore')]\")))\n",
    "    explore_menu.click()\n",
    "    trending_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'Trending')]\")))\n",
    "    trending_option.click()\n",
    "\n",
    "    # Wait for trending repositories to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//article[@class='Box-row']\")))\n",
    "\n",
    "    # Extracting repository details\n",
    "    repository_elements = driver.find_elements(By.XPATH, \"//article[@class='Box-row']\")\n",
    "\n",
    "    for repo in repository_elements:\n",
    "        title = repo.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "        description = repo.find_element(By.TAG_NAME, \"p\").text.strip()\n",
    "        try:\n",
    "            contributors_count = repo.find_element(By.XPATH, \".//a[contains(@href, '/contributors/')]\").text.strip()\n",
    "        except:\n",
    "            contributors_count = \"No contributors listed\"\n",
    "        try:\n",
    "            language = repo.find_element(By.XPATH, \".//span[@itemprop='programmingLanguage']\").text.strip()\n",
    "        except:\n",
    "            language = \"No specific language mentioned\"\n",
    "        \n",
    "        print(\"Repository Title:\", title)\n",
    "        print(\"Description:\", description)\n",
    "        print(\"Contributors Count:\", contributors_count)\n",
    "        print(\"Language Used:\", language)\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "# URL of the GitHub website\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Call the function to scrape trending repositories\n",
    "scrape_trending_repositories(url)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993cb7ab-15b6-48ca-b68f-a2167b8f5860",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to handle commonly occurring exceptions\n",
    "def handle_exceptions(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to scrape top 100 songs from Billboard.com\n",
    "@handle_exceptions\n",
    "def scrape_top_100_songs(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Click on the Charts menu and then on Hot 100\n",
    "    charts_menu = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//div[@id='root']//a[@class='header__main-link'][contains(text(), 'Charts')]\")))\n",
    "    charts_menu.click()\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'Hot 100')]\")))\n",
    "    hot_100_link.click()\n",
    "\n",
    "    # Wait for the chart to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//ol[@class='chart-list__elements']\")))\n",
    "\n",
    "    # Extracting song details\n",
    "    song_elements = driver.find_elements(By.XPATH, \"//li[@class='chart-list__element display--flex']\")\n",
    "\n",
    "    for song in song_elements:\n",
    "        song_name = song.find_element(By.XPATH, \".//span[@class='chart-element__information__song text--truncate color--primary']\").text.strip()\n",
    "        artist_name = song.find_element(By.XPATH, \".//span[@class='chart-element__information__artist text--truncate color--secondary']\").text.strip()\n",
    "        try:\n",
    "            last_week_rank = song.find_element(By.XPATH, \".//span[@class='chart-element__meta text--center color--secondary text--last']\").text.strip()\n",
    "        except:\n",
    "            last_week_rank = \"N/A\"\n",
    "        peak_rank = song.find_element(By.XPATH, \".//span[@class='chart-element__meta text--center color--secondary text--peak']\").text.strip()\n",
    "        weeks_on_board = song.find_element(By.XPATH, \".//span[@class='chart-element__meta text--center color--secondary text--week']\").text.strip()\n",
    "\n",
    "        print(\"Song Name:\", song_name)\n",
    "        print(\"Artist Name:\", artist_name)\n",
    "        print(\"Last Week Rank:\", last_week_rank)\n",
    "        print(\"Peak Rank:\", peak_rank)\n",
    "        print(\"Weeks on Board:\", weeks_on_board)\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "# URL of the Billboard website\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Call the function to scrape top 100 songs\n",
    "scrape_top_100_songs(url)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391941dc-6ade-4fb6-a643-7d291b72c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to handle commonly occurring exceptions\n",
    "def handle_exceptions(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "# Function to scrape details of highest selling novels\n",
    "@handle_exceptions\n",
    "def scrape_highest_selling_novels(url):\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Checking if request was successful\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Finding the table containing the novel details\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extracting data from the table\n",
    "    novel_details = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        book_name = columns[0].text.strip()\n",
    "        author_name = columns[1].text.strip()\n",
    "        volumes_sold = columns[2].text.strip()\n",
    "        publisher = columns[3].text.strip()\n",
    "        genre = columns[4].text.strip()\n",
    "        novel_details.append({\n",
    "            \"Book Name\": book_name,\n",
    "            \"Author Name\": author_name,\n",
    "            \"Volumes Sold\": volumes_sold,\n",
    "            \"Publisher\": publisher,\n",
    "            \"Genre\": genre\n",
    "        })\n",
    "\n",
    "    # Print the scraped data\n",
    "    for novel in novel_details:\n",
    "        print(\"Book Name:\", novel[\"Book Name\"])\n",
    "        print(\"Author Name:\", novel[\"Author Name\"])\n",
    "        print(\"Volumes Sold:\", novel[\"Volumes Sold\"])\n",
    "        print(\"Publisher:\", novel[\"Publisher\"])\n",
    "        print(\"Genre:\", novel[\"Genre\"])\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Call the function to scrape highest selling novels\n",
    "scrape_highest_selling_novels(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422d9434-8630-4361-8dd2-9056baf44c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to handle commonly occurring exceptions\n",
    "def handle_exceptions(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "# Function to scrape details of datasets from UCI Machine Learning Repository\n",
    "@handle_exceptions\n",
    "def scrape_datasets(url):\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Checking if request was successful\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Finding the table containing the dataset details\n",
    "    table = soup.find('table', {'border': '1', 'cellspacing': '0', 'cellpadding': '5', 'width': '90%'})\n",
    "\n",
    "    # Extracting data from the table\n",
    "    dataset_details = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        dataset_name = columns[0].text.strip()\n",
    "        data_type = columns[1].text.strip()\n",
    "        task = columns[2].text.strip()\n",
    "        attribute_type = columns[3].text.strip()\n",
    "        num_instances = columns[4].text.strip()\n",
    "        num_attributes = columns[5].text.strip()\n",
    "        year = columns[6].text.strip()\n",
    "        dataset_details.append({\n",
    "            \"Dataset Name\": dataset_name,\n",
    "            \"Data Type\": data_type,\n",
    "            \"Task\": task,\n",
    "            \"Attribute Type\": attribute_type,\n",
    "            \"No of Instances\": num_instances,\n",
    "            \"No of Attributes\": num_attributes,\n",
    "            \"Year\": year\n",
    "        })\n",
    "\n",
    "    # Print the scraped data\n",
    "    for dataset in dataset_details:\n",
    "        print(\"Dataset Name:\", dataset[\"Dataset Name\"])\n",
    "        print(\"Data Type:\", dataset[\"Data Type\"])\n",
    "        print(\"Task:\", dataset[\"Task\"])\n",
    "        print(\"Attribute Type:\", dataset[\"Attribute Type\"])\n",
    "        print(\"No of Instances:\", dataset[\"No of Instances\"])\n",
    "        print(\"No of Attributes:\", dataset[\"No of Attributes\"])\n",
    "        print(\"Year:\", dataset[\"Year\"])\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Call the function to scrape dataset details\n",
    "scrape_datasets(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2be65b-ec7c-4cb2-88fe-8a17d1230fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
