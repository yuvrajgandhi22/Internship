{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dfbdef-44b7-4789-8e9a-3f6164fc8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    # Construct the URL for the search query\n",
    "    url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}\"\n",
    "\n",
    "    # Send HTTP request to Amazon\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all product containers\n",
    "        products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "        \n",
    "        # Extract and print product details\n",
    "        for product in products:\n",
    "            product_title = product.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "            product_price = product.find(\"span\", class_=\"a-price-whole\").text.strip()\n",
    "            print(f\"Product: {product_title}, Price: {product_price}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Amazon.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt the user to input the product to search on Amazon\n",
    "    product_name = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "    # Perform the search and scrape product details\n",
    "    search_amazon(product_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba3fbb-c6e6-4336-846f-488725ff8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    " for page in range(1, num_pages + 1):\n",
    "        # Construct the URL for the search query\n",
    "        url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}&page={page}\"\n",
    "\n",
    "        # Send HTTP request to Amazon\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all product containers\n",
    "            products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "            \n",
    "            # Extract and append product details to the DataFrame\n",
    "            for product in products:\n",
    "                brand_name = product.find(\"span\", class_=\"a-size-base-plus a-color-base a-text-normal\")\n",
    "                product_title = product.find(\"span\", class_=\"a-text-normal\")\n",
    "                product_price = product.find(\"span\", class_=\"a-price\")\n",
    "                return_exchange = product.find(\"span\", class_=\"a-text-bold\")\n",
    "                expected_delivery = product.find(\"span\", class_=\"a-text-bold\")\n",
    "                availability = product.find(\"span\", class_=\"a-size-base a-color-success\")\n",
    "                product_url = product.find(\"a\", class_=\"a-link-normal\", href=True)\n",
    "                \n",
    "                if brand_name:\n",
    "                    brand_name = brand_name.text.strip()\n",
    "                else:\n",
    "                    brand_name = \"-\"\n",
    "                \n",
    "                if product_title:\n",
    "                    product_title = product_title.text.strip()\n",
    "                else:\n",
    "                    product_title = \"-\"\n",
    "                \n",
    "                if product_price:\n",
    "                    product_price = product_price.find(\"span\", class_=\"a-offscreen\").text.strip()\n",
    "                else:\n",
    "                    product_price = \"-\"\n",
    "                \n",
    "                if return_exchange:\n",
    "                    return_exchange = return_exchange.text.strip()\n",
    "                else:\n",
    "                    return_exchange = \"-\"\n",
    "                \n",
    "                if expected_delivery:\n",
    "                    expected_delivery = expected_delivery.text.strip()\n",
    "                else:\n",
    "                    expected_delivery = \"-\"\n",
    "                \n",
    "                if availability:\n",
    "                    availability = availability.text.strip()\n",
    "                else:\n",
    "                    availability = \"-\"\n",
    "                \n",
    "                if product_url:\n",
    "                    product_url = \"https://www.amazon.in\" + product_url['href']\n",
    "                else:\n",
    "                    product_url = \"-\"\n",
    "                \n",
    "                df = df.append({\n",
    "                    \"Brand Name\": brand_name,\n",
    "                    \"Name of the Product\": product_title,\n",
    "                    \"Price\": product_price,\n",
    "                    \"Return/Exchange\": return_exchange,\n",
    "                    \"Expected Delivery\": expected_delivery,\n",
    "                    \"Availability\": availability,\n",
    "                    \"Product URL\": product_url\n",
    "                }, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from Amazon for page {page}.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt the user to input the product to search on Amazon\n",
    "    product_name = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "    # Perform the search and scrape product details\n",
    "    df = search_amazon(product_name)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f\"{product_name}_products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31999c8-8891-4d6d-8993-dfb93aed5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    url = f\"https://www.google.com/search?tbm=isch&q={keyword.replace(' ', '+')}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more images\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Extract image URLs\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    image_tags = soup.find_all('img', class_='rg_i')\n",
    "    \n",
    "    # Download images\n",
    "    count = 0\n",
    "    for image_tag in image_tags:\n",
    "        image_url = image_tag.get('src')\n",
    "        if image_url:\n",
    "            image_url = image_url.split('&')[0]  # Remove extra parameters\n",
    "            try:\n",
    "                image_data = requests.get(image_url, timeout=10).content\n",
    "                with open(f\"{keyword}_{count+1}.jpg\", 'wb') as f:\n",
    "                    f.write(image_data)\n",
    "                count += 1\n",
    "                if count >= num_images:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading image: {e}\")\n",
    "\n",
    "# Create a Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Keywords to search for\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    \n",
    "    # Number of images to scrape for each keyword\n",
    "    num_images = 10\n",
    "    \n",
    "    # Scrape images for each keyword\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for '{keyword}'...\")\n",
    "        scrape_images(keyword, num_images)\n",
    "        print(f\"Scraped {num_images} images for '{keyword}'\")\n",
    "        print()\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d089c-bbc5-4851-9d72-8ec188feca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_flipkart_smartphones(product_name):\n",
    "    url = f\"https://www.flipkart.com/search?q={product_name}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "    \n",
    "    # Send HTTP GET request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        data = []\n",
    "        for product in products:\n",
    "            try:\n",
    "                product_url = \"https://www.flipkart.com\" + product.find('a', class_='IRpwTa')['href']\n",
    "                brand_name, smartphone_name = product.find('div', class_='_4rR01T').text.split(' ')\n",
    "                other_details = product.find_all('li', class_='rgWa7D')\n",
    "\n",
    "                colour, ram, rom, primary_camera, secondary_camera, display_size, battery_capacity, price = ['-'] * 8\n",
    "\n",
    "                for detail in other_details:\n",
    "                    detail_text = detail.text.lower()\n",
    "                    if 'color' in detail_text:\n",
    "                        colour = detail_text.split(':')[1].strip()\n",
    "                    elif 'ram' in detail_text:\n",
    "                        ram = detail_text.split(':')[1].strip()\n",
    "                    elif 'rom' in detail_text:\n",
    "                        rom = detail_text.split(':')[1].strip()\n",
    "                    elif 'primary camera' in detail_text:\n",
    "                        primary_camera = detail_text.split(':')[1].strip()\n",
    "                    elif 'secondary camera' in detail_text:\n",
    "                        secondary_camera = detail_text.split(':')[1].strip()\n",
    "                    elif 'display size' in detail_text:\n",
    "                        display_size = detail_text.split(':')[1].strip()\n",
    "                    elif 'battery capacity' in detail_text:\n",
    "                        battery_capacity = detail_text.split(':')[1].strip()\n",
    "\n",
    "                data.append({\n",
    "                    'Brand Name': brand_name,\n",
    "                    'Smartphone Name': smartphone_name,\n",
    "                    'Colour': colour,\n",
    "                    'RAM': ram,\n",
    "                    'Storage(ROM)': rom,\n",
    "                    'Primary Camera': primary_camera,\n",
    "                    'Secondary Camera': secondary_camera,\n",
    "                    'Display Size': display_size,\n",
    "                    'Battery Capacity': battery_capacity,\n",
    "                    'Price': product.find('div', class_='_30jeq3').text,\n",
    "                    'Product URL': product_url\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping product details: {e}\")\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Flipkart.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the smartphone name to search on Flipkart: \")\n",
    "    \n",
    "    # Scrape data\n",
    "    data = scrape_flipkart_smartphones(product_name)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"{product_name}_flipkart_smartphones.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1609d-7b2c-4d56-aff5-a82df1b771d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    # Create a Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Open Google Maps\n",
    "        driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "        # Find the search bar and enter the city name\n",
    "        search_box = driver.find_element_by_id(\"searchboxinput\")\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(city_name)\n",
    "        search_box.submit()\n",
    "\n",
    "        # Wait for the URL to update\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Get the current URL which contains the coordinates\n",
    "        url = driver.current_url\n",
    "\n",
    "        # Extract latitude and longitude from the URL\n",
    "        lat_lng_index = url.index('@') + 1\n",
    "        comma_index = url.index(',', lat_lng_index)\n",
    "        latitude = url[lat_lng_index:comma_index]\n",
    "        longitude = url[comma_index + 1:url.index(',', comma_index + 1)]\n",
    "\n",
    "        print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the webdriver\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city name to search on Google Maps: \")\n",
    "    get_coordinates(city_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bee33-385e-43e2-8ba6-bd2270e957b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops = []\n",
    "        laptop_list = soup.find('div', class_='TopNumbeHeading active-tabs')\n",
    "        if laptop_list:\n",
    "            laptops_html = laptop_list.find_next_sibling('ul').find_all('li')\n",
    "            for laptop_html in laptops_html:\n",
    "                laptop = {}\n",
    "                laptop['Name'] = laptop_html.find('div', class_='TopNumbeHeading').text.strip()\n",
    "                specs_html = laptop_html.find('div', class_='Specs').find_all('div', class_='value')\n",
    "                laptop['Processor'] = specs_html[0].text.strip()\n",
    "                laptop['RAM'] = specs_html[1].text.strip()\n",
    "                laptop['OS'] = specs_html[2].text.strip()\n",
    "                laptop['Display'] = specs_html[3].text.strip()\n",
    "                laptop['Memory'] = specs_html[4].text.strip()\n",
    "                laptop['Price'] = laptop_html.find('div', class_='smprice').text.strip()\n",
    "                laptops.append(laptop)\n",
    "\n",
    "        return laptops\n",
    "    else:\n",
    "        print(\"Failed to fetch data from digit.in.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops:\n",
    "        print(\"Gaming Laptops:\")\n",
    "        for laptop in gaming_laptops:\n",
    "            print(laptop)\n",
    "    else:\n",
    "        print(\"No gaming laptops found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccd303-559a-4a7c-b02e-a654657b6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        billionaires = []\n",
    "        table = soup.find('div', class_='table-body')\n",
    "        rows = table.find_all('div', class_='table-row')\n",
    "\n",
    "        for row in rows:\n",
    "            rank = row.find('div', class_='rank').text.strip()\n",
    "            name = row.find('div', class_='personName').text.strip()\n",
    "            net_worth = row.find('div', class_='netWorth').text.strip()\n",
    "            age = row.find('div', class_='age').text.strip()\n",
    "            citizenship = row.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = row.find('div', class_='source').text.strip()\n",
    "            industry = row.find('div', class_='category').text.strip()\n",
    "\n",
    "            billionaire = {\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net Worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            }\n",
    "            billionaires.append(billionaire)\n",
    "\n",
    "        return billionaires\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Forbes.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires = scrape_billionaires()\n",
    "\n",
    "    if billionaires:\n",
    "        print(\"Billionaires:\")\n",
    "        for billionaire in billionaires:\n",
    "            print(billionaire)\n",
    "    else:\n",
    "        print(\"No billionaires found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5cdcf-eaa1-404c-bbfa-b52a92e86475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up the YouTube Data API client\n",
    "api_key = \"YOUR_API_KEY\"  # Replace with your own API key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < 500:\n",
    "        # Fetch comments for the video\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Maximum 100 comments per page\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "            comment_text = comment[\"snippet\"][\"textDisplay\"]\n",
    "            comment_upvotes = comment[\"snippet\"][\"likeCount\"]\n",
    "            comment_time = comment[\"snippet\"][\"publishedAt\"]\n",
    "            comment_time = datetime.strptime(comment_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            comments.append({\n",
    "                \"Comment\": comment_text,\n",
    "                \"Upvotes\": comment_upvotes,\n",
    "                \"Time\": comment_time\n",
    "            })\n",
    "\n",
    "        # Check if there are more pages of comments\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    video_comments = get_video_comments(video_id)\n",
    "\n",
    "    print(f\"Total Comments: {len(video_comments)}\")\n",
    "    for comment in video_comments:\n",
    "        print(f\"Comment: {comment['Comment']}\")\n",
    "        print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "        print(f\"Time: {comment['Time']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329e839-1a94-4037-a1bb-4b521c910e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = 'https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        hostels = []\n",
    "        hostel_cards = soup.find_all('div', class_='fabresult rounded clearfix hwta-property')\n",
    "\n",
    "        for card in hostel_cards:\n",
    "            name = card.find('a', class_='hwta-property-link').text.strip()\n",
    "            distance = card.find('span', class_='description').text.strip()\n",
    "            ratings = card.find('div', class_='score orange big').text.strip()\n",
    "            total_reviews = card.find('div', class_='reviews').text.strip()\n",
    "            overall_reviews = card.find('div', class_='reviews').find('span').get('title')\n",
    "            privates_price = card.find('div', class_='prices').find('a', class_='prices').text.strip()\n",
    "            dorms_price = card.find('div', class_='prices').find('a', class_='dorms').text.strip()\n",
    "            facilities = [facility.text.strip() for facility in card.find_all('span', class_='facilities')]\n",
    "            description = card.find('div', class_='additional-info').text.strip()\n",
    "\n",
    "            hostel = {\n",
    "                'Name': name,\n",
    "                'Distance from City Centre': distance,\n",
    "                'Ratings': ratings,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates from Price': privates_price,\n",
    "                'Dorms from Price': dorms_price,\n",
    "                'Facilities': facilities,\n",
    "                'Description': description\n",
    "            }\n",
    "            hostels.append(hostel)\n",
    "\n",
    "        return hostels\n",
    "    else:\n",
    "        print(\"Failed to fetch data from hostelworld.com.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    london_hostels = scrape_hostels_in_london()\n",
    "\n",
    "    if london_hostels:\n",
    "        print(\"London Hostels:\")\n",
    "        for hostel in london_hostels:\n",
    "            print(hostel)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No hostels found in London.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a54ff4-ae18-4e58-afd5-21983a1f9217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
